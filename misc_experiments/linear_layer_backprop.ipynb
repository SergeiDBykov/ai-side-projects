{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7897,  0.4034, -1.1643]) tensor([0.8723]) tensor([0.5053], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(3, 3)\n",
    "        self.fc1 = nn.Linear(3, 4)\n",
    "        self.fc2 = nn.Linear(4, 5)\n",
    "        self.fc3 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f0 = self.fc0(x)\n",
    "        h1 = F.relu(f0)\n",
    "        f1 = self.fc1(h1)\n",
    "        h2 = F.relu(f1)\n",
    "        f2 = self.fc2(h2)\n",
    "        h3 = F.relu(f2)\n",
    "        f3 = self.fc3(h3)\n",
    "        x = f3\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "input = torch.randn(3)\n",
    "target = torch.randn(1)\n",
    "\n",
    "net = Net()\n",
    "pred = net(input)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "#get gradients\n",
    "optimizer.zero_grad()\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net output my / pytorch:\n",
      "tensor([0.5053], grad_fn=<ReluBackward0>) tensor([0.5053], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#manual forward pass\n",
    "\n",
    "layers_input = []\n",
    "layers_preactivations = []\n",
    "layers_outputs = []\n",
    "\n",
    "layers_weights = []\n",
    "layers_biases = []\n",
    "\n",
    "ws = [net.fc0.weight, net.fc1.weight, net.fc2.weight, net.fc3.weight]\n",
    "bs = [net.fc0.bias, net.fc1.bias, net.fc2.bias, net.fc3.bias]\n",
    "\n",
    "input_next = input\n",
    "for k in range(4):\n",
    "    layers_input.append(input_next)\n",
    "    w = ws[k]\n",
    "    b = bs[k]\n",
    "    f = torch.matmul(w, input_next) + b\n",
    "    h = F.relu(f)\n",
    "    input_next = h\n",
    "\n",
    "    layers_preactivations.append(f)\n",
    "    layers_outputs.append(h)\n",
    "\n",
    "    layers_weights.append(w)\n",
    "    layers_biases.append(b)\n",
    "\n",
    "    #print(f, fs[k])\n",
    "    #print(h, hs[k]) if k <3 else None\n",
    "\n",
    "print('Net output my / pytorch:')\n",
    "print(layers_outputs[-1], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===layer k=0===\n",
      "grad my/torch:\n",
      "[[ 0.04533107 -0.00655478  0.01891906]\n",
      " [ 0.13311084 -0.01924756  0.0555542 ]\n",
      " [-0.          0.         -0.        ]]\n",
      "[[ 0.04533107 -0.00655478  0.01891906]\n",
      " [ 0.13311084 -0.01924756  0.0555542 ]\n",
      " [-0.          0.         -0.        ]]\n",
      "bias my/torch::\n",
      "[-0.01624922 -0.04771445  0.        ]\n",
      "[-0.01624922 -0.04771445  0.        ]\n",
      "===layer k=1===\n",
      "grad my/torch:\n",
      "[[-0.13762619 -0.13665056 -0.        ]\n",
      " [-0.         -0.         -0.        ]\n",
      " [-0.11894695 -0.11810374 -0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "[[-0.13762619 -0.13665056 -0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.11894695 -0.11810374 -0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "bias my/torch::\n",
      "[-0.08385601 -0.         -0.0724747   0.        ]\n",
      "[-0.08385601  0.         -0.0724747   0.        ]\n",
      "===layer k=2===\n",
      "grad my/torch:\n",
      "[[-0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.        ]\n",
      " [-0.13080193 -0.         -0.05091183 -0.        ]\n",
      " [ 0.14241843  0.          0.05543331  0.        ]\n",
      " [-0.25361982 -0.         -0.09871605 -0.        ]]\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.13080193 -0.         -0.05091183 -0.        ]\n",
      " [ 0.14241843  0.          0.05543331  0.        ]\n",
      " [-0.25361982 -0.         -0.09871605 -0.        ]]\n",
      "bias my/torch::\n",
      "[-0.         -0.         -0.11896957  0.12953524 -0.23067734]\n",
      "[ 0.          0.         -0.11896957  0.12953524 -0.23067734]\n",
      "===layer k=3===\n",
      "grad my/torch:\n",
      "[[ 0.          0.         -0.28927052 -0.326482   -0.23299772]]\n",
      "[[ 0.          0.         -0.28927052 -0.326482   -0.23299772]]\n",
      "bias my/torch::\n",
      "[-0.73408866]\n",
      "[-0.73408866]\n"
     ]
    }
   ],
   "source": [
    "# manual backward pass\n",
    "\n",
    "grads = []\n",
    "grads_ws = []\n",
    "grads_b2 = []\n",
    "\n",
    "for k in [3,2,1,0]:\n",
    "\n",
    "    if k == 3:\n",
    "        grad = 2*(layers_outputs[k] - target) #mse loss derivative, d(x-y)^2\n",
    "    else:\n",
    "        grad = torch.matmul(layers_weights[k+1].T, grads[-1]) #dl/df(k) = omega_(k+1)^T * dl/df(k+1), grads[-1] is the previous layer's gradient\n",
    "        grad = grad * (layers_preactivations[k] > 0).float() #for relu, zero out gradients where preactivation is negative\n",
    "\n",
    "    grads.append(grad)\n",
    "\n",
    "    grad_w = torch.matmul(grad.view(-1,1), layers_input[k].view(1,-1)) #dl/dw(k) = dl/df(k) * h(k). tensor.view(-1, 1) makes it a column vector instead of a row vector\n",
    "\n",
    "    grad_b = grad\n",
    "\n",
    "    grads_ws.append(grad_w)\n",
    "    grads_b2.append(grad_b)\n",
    "\n",
    "\n",
    "grads = grads[::-1]\n",
    "grads_ws = grads_ws[::-1]\n",
    "grads_b2 = grads_b2[::-1]\n",
    "\n",
    "\n",
    "for k in range(4):\n",
    "    print(f'===layer {k=}===')\n",
    "    my_weight_grad = grads_ws[k]\n",
    "    my_bias_grad = grads_b2[k]\n",
    "    torch_weight_grad = layers_weights[k].grad\n",
    "    torch_bias_grad = layers_biases[k].grad\n",
    "\n",
    "    my_grad_np = my_weight_grad.detach().numpy()\n",
    "    torch_grad_np = torch_weight_grad.detach().numpy()\n",
    "\n",
    "    print('grad my/torch:')\n",
    "    print(my_grad_np)\n",
    "    print(torch_grad_np)\n",
    "\n",
    "    my_grad_np = my_bias_grad.detach().numpy()\n",
    "    torch_grad_np = torch_bias_grad.detach().numpy()\n",
    "\n",
    "    print('bias my/torch::')\n",
    "    print(my_grad_np)\n",
    "    print(torch_grad_np)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
